#loading required libraries
library(caret);library(ggplot2);library(lattice);library(rpart);library(rpart.plot);library(survival);library(parallel)
library(splines);library(gbm);library(randomForest);library(plyr);library(adabag);library(mlbench)

#Reading the training and testing urls
trainingurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testingurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"


#Reading the csv files
training <- read.csv(trainingurl, na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(testingurl, na.strings=c("NA","#DIV/0!",""))


#Removing variables with zero variances
zerovar <- nearZeroVar(training, saveMetrics=TRUE)
training <- training[,!zerovar$nzv]


#Removing unused variable, columns 1 to 6 contain username and timestamps
training <- training[,-c(1:6)]


#Removing columns that are mostly NAs
is.data <- (apply(!is.na(training),2,sum)>= .7*nrow(training))
training <- training[,is.data]


#partitioning the data into training and testing parts
set.seed(1234)
inTrain <- createDataPartition(training$classe, p =.7, list=FALSE)
trainset <- training[inTrain,]
testset <- training[-inTrain,]

#Prediction with adaboost bagging method (adabag)
adabagmod <- train(classe ~., data=trainset, method="AdaBag")
predadabag <- predict(adabagmod, testset)
confusionMatrix(predadabag, testset$classe)
confadabag <- confusionMatrix(predadabag, testset$classe)

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1639  752  995  800  230
         B   35  387   31  164   90
         C    0    0    0    0    0
         D    0    0    0    0    0
         E    0    0    0    0  244

Overall Statistics
                                          
               Accuracy : 0.423           
                 95% CI : (0.4097, 0.4363)
    No Information Rate : 0.3119          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.188           
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9791  0.33977   0.0000   0.0000  0.43262
Specificity            0.2480  0.92431   1.0000   1.0000  1.00000
Pos Pred Value         0.3712  0.54738      NaN      NaN  1.00000
Neg Pred Value         0.9632  0.83863   0.8088   0.8204  0.93754
Prevalence             0.3119  0.21222   0.1912   0.1796  0.10509
Detection Rate         0.3054  0.07211   0.0000   0.0000  0.04546
Detection Prevalence   0.8228  0.13173   0.0000   0.0000  0.04546
Balanced Accuracy      0.6136  0.63204   0.5000   0.5000  0.71631


#prediction with Gradient Boosting Method (gbm)
gbmmod <- train(classe ~., method ="gbm", data = trainset, verbose=FALSE)
predgbm <- predict(gbmmod, testset)
confgbm <- confusionMatrix(predgbm,testset$classe)

#prediction with recursive partitioning and regresssion trees (rpart) method
rpartmod <- rpart(classe~., data=trainset)
rpart.plot(rpartmod, extra = 100)
predrpart <- predict(rpartmod, testset, type="class")
confrpart <- confusionMatrix(predrpart, testset$classe)


#prediction using randomForest (rf)
rfmod <- randomForest(classe ~., data=trainset, method="class")
predrf <- predict(rfmod, testset, type="class")
confrf <- confusionMatrix(predrf, testset$classe)

#creating a table to compare accuracies
method <- c("rpart", "rf", "adabag","gbm")
accuracy <- c(confrpart$overall[1],confrf$overall[1],confadabag$overall[1],confgbm$overall[1])
accuracytable <- data.frame(method,accuracy)
accuracytable

#making the final prediction with rf, since it has the highest accuracy
predfinal <- predict(rfmod, testing, type="class")
predfinal
